INVERTED INDEX

import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
stopWords = set(stopwords.words('english'))

document1 = "The quick brown fox jumped over the lazy dog"
document2 = "The lazy dog slept in the sun"

tokens1 = [word for word in document1.lower().split() if word not in stopWords]
tokens2 = [word for word in document2.lower().split() if word not in stopWords]

unique_tokens = list(set(tokens1 + tokens2))

inverted_index = {}
occ_num_doc1 = {}
occ_num_doc2 = {}

for term in unique_tokens:
    documents = []
    if term in tokens1:
        documents.append("Document 1")
        occ_num_doc1[term] = tokens1.count(term)
    if term in tokens2:
        documents.append("Document 2")
        occ_num_doc2[term] = tokens2.count(term)
    inverted_index[term] = documents

for term, documents in inverted_index.items():
    print(term, "->", end=" ")
    for doc in documents:
        if doc == "Document 1":
            print(f"{doc} ({occ_num_doc1.get(term, 0)}),", end=" ")
        else:
            print(f"{doc} ({occ_num_doc2.get(term, 0)}),", end=" ")
    print()

=======================================================================================

BOOLEAN RETRIEVAL MODEL

documents = {
    1: "apple banana orange",
    2: "apple banana",
    3: "banana orange",
    4: "apple"
}

def build_index(docs):
    index = {}  # Initialize an empty dictionary to store the inverted index
    for doc_id, text in docs.items():  # Iterate through each document and its text
        terms = set(text.split())  # Split the text into individual terms
        for term in terms:  # Iterate through each term in the document
            if term not in index:
                index[term] = {doc_id}  # If the term is not in the index, create a new set with document ID
            else:
                index[term].add(doc_id)  # If the term exists, add the document ID to its set
    return index  # Return the built inverted index

inverted_index = build_index(documents)

def boolean_and(operands, index):
    if not operands:  # If there are no operands, return all document IDs
        return list(range(1, len(documents) + 1))
    result = index.get(operands[0], set())  # Get the set of document IDs for the first operand
    for term in operands[1:]:  # Iterate through the rest of the operands
        result = result.intersection(index.get(term, set()))  # Compute intersection with sets of document IDs
    return list(result)  # Return the resulting list of document IDs

def boolean_or(operands, index):
    result = set()  # Initialize an empty set to store the resulting document IDs
    for term in operands:
        result = result.union(index.get(term, set()))
    return list(result)  # Return the resulting list of document IDs

def boolean_not(operand, index, total_docs):
    operand_set = set(index.get(operand, set()))  # Get the set of document IDs for the operand
    all_docs_set = set(range(1, total_docs + 1))  # Create a set of all document IDs
    return list(all_docs_set.difference(operand_set))  # Return documents not in the operand set

query1 = ["apple", "banana"]  # Query for documents containing both "apple" and "banana"
query2 = ["apple", "orange"]  # Query for documents containing "apple" or "orange"
result1 = boolean_and(query1, inverted_index)  # Get documents containing both terms
result2 = boolean_or(query2, inverted_index)  # Get documents containing either of the terms
result3 = boolean_not("orange", inverted_index, len(documents))  # Get documents not containing "orange"

print("Documents containing 'apple' and 'banana':", result1)
print("Documents containing 'apple' or 'orange':", result2)
print("Documents not containing 'orange':", result3)


========================================================================================================================

VECTOR SPACE MODEL WITH TF-IDF WEIGHTING AND COSINESIMILARITY

from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer 
import nltk 
from nltk.corpus import stopwords 
from numpy.linalg import norm 

# Download stopwords from NLTK
nltk.download('stopwords')

# Define the train set and test set
train_set = ["The sky is blue.", "The sun is bright."]
test_set = ["The sun in the sky is bright."]  # Query

# Load English stopwords
stopWords = stopwords.words('english')

# Initialize CountVectorizer with stopwords
vectorizer = CountVectorizer(stop_words=stopWords)

# Initialize TfidfTransformer
transformer = TfidfTransformer()

# Fit and transform the training set
trainVectorizerArray = vectorizer.fit_transform(train_set).toarray()

# Transform the test set
testVectorizerArray = vectorizer.transform(test_set).toarray()

print('Fit Vectorizer to train set', trainVectorizerArray)
print('Transform Vectorizer to test set', testVectorizerArray)

# Display the vector for each document in the test set
print("Train Vectorizer Array:")
for vector in trainVectorizerArray:
    print(vector)

print("Test Vectorizer Array:")
for test_vector in testVectorizerArray:
    print(test_vector)

# Calculate cosine similarity between the vectors
cosine = lambda v1, v2: np.dot(v1, v2) / (norm(v1) * norm(v2))

# Fit the transformer on the training set and transform it
transformer.fit(trainVectorizerArray)
train_tfidf = transformer.transform(trainVectorizerArray).toarray()

# Fit the transformer on the test set and transform it
transformer.fit(testVectorizerArray)
test_tfidf = transformer.transform(testVectorizerArray).toarray()

print()
print("TF-IDF representation for train set:")
print(train_tfidf)

print()
print("TF-IDF representation for test set:")
print(test_tfidf)


=========================================================================================================

SPELLING CORRECTION

def editDistance(str1, str2, m, n):
    # If first string is empty, the only option is to insert all characters of second string into the first
    if m == 0:
        return n
    # If second string is empty, the only option is to remove all characters of first string
    if n == 0:
        return m
    
    # If last characters of two strings are the same, ignore them and recur for remaining strings
    if str1[m-1] == str2[n-1]:
        return editDistance(str1, str2, m-1, n-1)
    
    # If last characters are not the same, consider all three operations on last character of first string,
    # recursively compute minimum cost for all three operations and take minimum of three values
    return 1 + min(editDistance(str1, str2, m, n-1),  # Insert
                   editDistance(str1, str2, m-1, n),  # Remove
                   editDistance(str1, str2, m-1, n-1) # Replace
                  )

# Driver code
str1 = "sunday"
str2 = "saturday"
print('Edit Distance is:', editDistance(str1, str2, len(str1), len(str2)))


===========================================================================================================

CALCULATE RECALL, PRECISION ,F-MEASURE

def calculate_metrics(retrieved_set, relevant_set):
    true_positive = len(retrieved_set.intersection(relevant_set))
    false_positive = len(retrieved_set.difference(relevant_set))
    false_negative = len(relevant_set.difference(retrieved_set))

    print("True Positive: ", true_positive,
          "\nFalse Positive: ", false_positive,
          "\nFalse Negative: ", false_negative, "\n")

    precision = true_positive / (true_positive + false_positive)
    recall = true_positive / (true_positive + false_negative)
    f_measure = 2 * precision * recall / (precision + recall)

    return precision, recall, f_measure

retrieved_set = set(["doc1", "doc2", "doc3"])  # Predicted set
relevant_set = set(["doc1", "doc4"])  # Actually Needed set (Relevant)

precision, recall, f_measure = calculate_metrics(retrieved_set, relevant_set)
print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F-measure: {f_measure}")

=================================================================================================================

EVALUATION TOOLKIT TO MEASURE AVERAGE PRECISION AND OTHER EVALUATION METRICES

from sklearn.metrics import average_precision_score

y_true = [0, 1, 1, 0, 1, 1]  # Binary Prediction
y_scores = [0.1, 0.4, 0.35, 0.8, 0.65, 0.9]  # Model's estimation score

average_precision = average_precision_score(y_true, y_scores)
print(f'Average precision-recall score: {average_precision}')

===================================================================================================================

TEXT CLASSIFICATION DATASET REQUIRED

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, classification_report

# Read the dataset
df = pd.read_csv(r"C:\Users\Administrator\Documents\Sem 6\IR\Dataset.csv")

# Concatenate "covid" and "fever" columns to form text data
data = df["covid"] + " " + df["fever"]
X = data.astype(str) # Convert to string
y = df['flu'] # Target variable

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize CountVectorizer
vectorizer = CountVectorizer()

# Fit and transform the training data
X_train_counts = vectorizer.fit_transform(X_train)

# Transform the testing data
X_test_counts = vectorizer.transform(X_test)

# Initialize and train the Naive Bayes classifier
classifier = MultinomialNB()
classifier.fit(X_train_counts, y_train)

# Load the test data
data1 = pd.read_csv(r"C:\Users\Administrator\Documents\Sem 6\IR\Test.csv")

# Concatenate "covid" and "fever" columns to form new data
new_data = data1["covid"] + " " + data1["fever"]

# Transform the new data using the same vectorizer
new_data_counts = vectorizer.transform(new_data.astype(str))

# Predict using the trained classifier
predictions = classifier.predict(new_data_counts)

# Print the predictions
print("Predictions:")
print(predictions)

# Calculate accuracy
accuracy = accuracy_score(y_test, classifier.predict(X_test_counts))
print(f"\nAccuracy: {accuracy:.2f}")

# Print classification report
print("Classification Report:")
print(classification_report(y_test, classifier.predict(X_test_counts)))

# Save predictions along with test data to a new CSV file
predictions_df = pd.DataFrame(predictions, columns=['flu_prediction'])
result_df = pd.concat([data1, predictions_df], axis=1)
result_df.to_csv(r"C:\Users\Administrator\Documents\Sem 6\IR\Test1.csv", index=False)

====================================================================================================================

CLUSTERING ALGORITHM TO A SET OF DOCUMENTS

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans

documents = [
    "Cats are known for their agility and grace",  # cat doc1
    "Dogs are often called ‘man’s best friend’.",  # dog doc1
    "Some dogs are trained to assist people with disabilities.",  # dog doc2
    "The sun rises in the east and sets in the west.",  # sun doc1
    "Many cats enjoy climbing trees and chasing toys.",  # cat doc2
]

# Create a TfidfVectorizer object
vectorizer = TfidfVectorizer(stop_words='english')

# Learn vocabulary and idf from training set
X = vectorizer.fit_transform(documents)

# Perform k-means clustering
kmeans = KMeans(n_clusters=3, random_state=0).fit(X)

# Print cluster labels for each document
print(kmeans.labels_)

======================================================================================================================

WEB CRAWLER

import requests
from bs4 import BeautifulSoup
import time
from urllib.parse import urljoin
from urllib.robotparser import RobotFileParser

def get_html(url):
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}
    try:
        response = requests.get(url, headers=headers)
        response.raise_for_status()
        return response.text
    except requests.exceptions.HTTPError as errh:
        print(f"HTTP Error: {errh}")
    except requests.exceptions.RequestException as err:
        print(f"Request Error: {err}")
    return None

def save_robots_txt(url):
    try:
        robots_url = urljoin(url, '/robots.txt')
        robots_content = get_html(robots_url)
        if robots_content:
            with open('robots.txt', 'wb') as file:
                file.write(robots_content.encode('utf-8-sig'))
    except Exception as e:
        print(f"Error saving robots.txt: {e}")

def load_robots_txt():
    try:
        with open('robots.txt', 'rb') as file:
            return file.read().decode('utf-8-sig')
    except FileNotFoundError:
        return None

def extract_links(html, base_url):
    soup = BeautifulSoup(html, 'html.parser')
    links = []
    for link in soup.find_all('a', href=True):
        absolute_url = urljoin(base_url, link['href'])
        links.append(absolute_url)
    return links

def is_allowed_by_robots(url, robots_content):
    parser = RobotFileParser()
    parser.parse(robots_content.split('\n'))
    return parser.can_fetch('*', url)

def crawl(start_url, max_depth=3, delay=1):
    visited_urls = set()
    def recursive_crawl(url, depth, robots_content):
        if depth > max_depth or url in visited_urls or not is_allowed_by_robots(url, robots_content):
            return
        visited_urls.add(url)
        time.sleep(delay)
        html = get_html(url)
        if html:
            print(f"Crawling {url}")
            links = extract_links(html, url)
            for link in links:
                recursive_crawl(link, depth + 1, robots_content)

    save_robots_txt(start_url)
    robots_content = load_robots_txt()
    if not robots_content:
        print("Unable to retrieve robots.txt. Crawling without restrictions.")
    recursive_crawl(start_url, 1, robots_content)

# Example usage:
crawl('https://wikipedia.com', max_depth=2, delay=2)

========================================================================================================================

ROBOT.TXT

User-agent: *
Disallow: /private/
Disallow: /restricted/

=======================================================================================================================

PAGE RANK ALGORITHM

import numpy as np

def page_rank(graph, damping_factor=0.85, max_iterations=100, tolerance=1e-6):
    num_nodes = len(graph)
    page_ranks = np.ones(num_nodes) / num_nodes
    prev_page_ranks = np.copy(page_ranks)
    
    for _ in range(max_iterations):
        for node in range(num_nodes):
            incoming_links = [i for i, edges in enumerate(graph) if node in edges]
            page_ranks[node] = (1 - damping_factor) / num_nodes + damping_factor * sum(prev_page_ranks[link] / len(graph[link]) for link in incoming_links)
        if np.linalg.norm(page_ranks - prev_page_ranks, 2) < tolerance:
            break
        prev_page_ranks = np.copy(page_ranks)
        
    return page_ranks

if __name__ == "__main__":
    web_graph = [
        [1, 2],   # Node 0 has links to Node 1 and Node 2
        [0, 2],   # Node 1 has links to Node 0 and Node 2
        [0, 1],   # Node 2 has links to Node 0 and Node 1
        [1, 2],   # Node 3 has links to Node 1 and Node 2
    ]
    
    result = page_rank(web_graph)
    for i, pr in enumerate(result):
        print(f"Page {i}: {pr}")

